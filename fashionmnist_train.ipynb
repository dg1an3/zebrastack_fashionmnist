{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0683780d03ee1195a9e1ac19e401b8f6c3447ee82d0b15d335dacfb764b91f68"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from autologging import logged, traced, TRACE\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(name)s.%(funcName)s\\t%(message)s\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()\n",
    "train_images = train_images[:len(train_images)//1]\n",
    "test_images = test_images[:len(test_images)//1]\n",
    "logging.info(f\"train_images: {train_images.shape} {train_images.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "@logged\n",
    "def prepare_images(img_array:np.ndarray, sz=64):\n",
    "    from skimage.transform import resize\n",
    "    img_array = (img_array / 255.0).astype(np.float32)\n",
    "    prepare_images._log.info(f\"img_array: {img_array.shape} {img_array.dtype}\")    \n",
    "    img_array = [resize(img_array[n], (sz,sz)) for n in range(img_array.shape[0])]\n",
    "    img_array = np.reshape(img_array, (len(img_array),sz,sz,1))\n",
    "    prepare_images._log.info(f\"img_array: {img_array.shape} {img_array.dtype}\")\n",
    "    return img_array\n",
    "\n",
    "train_images = prepare_images(train_images)\n",
    "test_images = prepare_images(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from custom_layers import GaborPowerMap2D, LogSumExpPooling2D\n",
    "\n",
    "def create_encoder(sz=64, latent_dim=8, locally_connected_channels=2, act_func='softplus',\n",
    "        gabor_powermaps=True, logsumexp_pool=True):\n",
    "    \"\"\" creates the encoder side of the autoencoder, for the parameters sz and latent_dim\n",
    "    # Static parameters\n",
    "        sz (int): sz x sz input\n",
    "        latent_dim (int): gaussian dimensions\n",
    "        locally_connected_channels = 2\n",
    "    # Arguments\n",
    "        <none>\n",
    "    # Returns\n",
    "        retina: the input layer\n",
    "        encoder: the encoder model\n",
    "        shape: shape of last input layer\n",
    "        [z_mean, z_log_var, z]: tensors for latent space\n",
    "    \"\"\"\n",
    "\n",
    "    return Sequential([\n",
    "        Input(shape=(sz,sz,1), name='retina_{}'.format(sz)),\n",
    "\n",
    "        ##### V1 layers\n",
    "\n",
    "        GaborPowerMap2D(16, (5,5), name='v1_powmap') \n",
    "        if gabor_powermaps \n",
    "        else Conv2D(16, (5,5), name='v1_conv2d', activation=act_func, padding='same'),\n",
    "\n",
    "        LogSumExpPooling2D(name='v1_pool') \n",
    "        if logsumexp_pool \n",
    "        else MaxPooling2D((2,2), name='v1_maxpool', padding='same'),\n",
    "\n",
    "        SpatialDropout2D(0.1, name='v1_dropout'),\n",
    "\n",
    "        ##### V2 layers\n",
    "\n",
    "        GaborPowerMap2D(16, (3,3), name='v2_powmap') \n",
    "        if gabor_powermaps \n",
    "        else Conv2D(16, (3,3), name='v2_conv2d', activation=act_func, padding='same'),\n",
    "\n",
    "        LogSumExpPooling2D(name='v2_pool') \n",
    "        if logsumexp_pool \n",
    "        else MaxPooling2D((2,2), name='v2_maxpool', padding='same'),\n",
    "\n",
    "        ##### V4 layers\n",
    "\n",
    "        GaborPowerMap2D(32, (3,3), name='v4_powmap') \n",
    "        if gabor_powermaps \n",
    "        else Conv2D(32, (3,3), name='v4_conv2d', activation=act_func, padding='same'),\n",
    "\n",
    "        LogSumExpPooling2D(name='v4_pool') \n",
    "        if logsumexp_pool \n",
    "        else MaxPooling2D((2,2), name='v4_maxpool', padding='same'),\n",
    "\n",
    "        ##### IT Layers\n",
    "\n",
    "        Conv2D(32, (3,3), name='pit_conv2d', activation=act_func, padding='same'),\n",
    "        Conv2D(64, (3,3), name='cit_conv2d', activation=act_func, padding='same'),\n",
    "        LocallyConnected2D(locally_connected_channels, (3,3), name='ait_local',        \n",
    "            activation=act_func),\n",
    "        ActivityRegularization(l1=0.0e-4, l2=0.0e-4, name='ait_regular'),\n",
    "\n",
    "        ##### Pulvinar\n",
    "\n",
    "        # generate latent vector Q(z|X)\n",
    "        Flatten(name='pulvinar_flatten'),\n",
    "        Dense(latent_dim, name='pulvinar_dense', activation=act_func),\n",
    "        Dense(latent_dim+latent_dim, name='z_mean_log_var'),\n",
    "    ], name='v1_to_pulvinar_encoder')\n",
    "\n",
    "encoder = create_encoder()\n",
    "encoder.summary()\n",
    "\n",
    "retina = encoder.input\n",
    "print(retina)\n",
    "\n",
    "dense_shape = encoder.get_layer('ait_local').output_shape\n",
    "print(dense_shape)\n",
    "\n",
    "output = encoder.get_layer('z_mean_log_var')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decoder(dense_shape, latent_dim=8, locally_connected_channels=2,\n",
    "        act_func='softplus'):\n",
    "    \"\"\" creates the decoder side of the autoencoder, given the input shape\n",
    "    # Static parameters\n",
    "        sz (int): sz x sz input\n",
    "        latent_dim (int): gaussian dimensions\n",
    "        locally_connected_channels = 2\n",
    "    # Arguments\n",
    "        shape: first input layer shape\n",
    "    # Returns\n",
    "        decoder: the decoder model\n",
    "    \"\"\"\n",
    "\n",
    "    return Sequential([\n",
    "        Input(shape=(latent_dim,), name='z_sampling'),\n",
    "        Dense(dense_shape[1] * dense_shape[2] * dense_shape[3], \n",
    "            name='pulvinar_dense_back', activation=act_func),\n",
    "        Reshape((dense_shape[1], dense_shape[2], dense_shape[3]),\n",
    "            name='pulvinar_antiflatten'),\n",
    "\n",
    "        ##### IT retro Layers\n",
    "\n",
    "        ZeroPadding2D(padding=(1,1), name='ait_padding_back'),\n",
    "        LocallyConnected2D(locally_connected_channels, (3,3), name='ait_local_back',\n",
    "            activation=act_func),\n",
    "\n",
    "        ZeroPadding2D(padding=(1,1), name='cit_padding_back'),\n",
    "        Conv2DTranspose(64, (3,3), name='cit_conv2d_trans', \n",
    "                activation=act_func, padding='same'),\n",
    "        Conv2DTranspose(32, (3,3), name='pit_conv2d_trans', \n",
    "                activation=act_func, padding='same'),\n",
    "\n",
    "        #### V4 retro layers\n",
    "\n",
    "        Conv2DTranspose(32, (3,3), name='v4_conv2d_trans', \n",
    "                activation=act_func, padding='same'),\n",
    "        UpSampling2D((2,2), name='v4_upsample_back'),\n",
    "\n",
    "        #### V2 retro layers\n",
    "\n",
    "        Conv2DTranspose(16, (3,3), name='v2_conv2d_trans', \n",
    "                activation=act_func, padding='same'),\n",
    "        UpSampling2D((2,2), name='v2_upsample_back'),\n",
    "\n",
    "        #### V1 retro layers\n",
    "\n",
    "        Conv2D(1, (5,5), name='v1_conv2d_5x5_back', \n",
    "                            # activation='sigmoid', no sigmoid == return logits\n",
    "                            padding='same'),\n",
    "        UpSampling2D((2,2), name='v1_upsample_back'),\n",
    "    ], name='pulvinar_to_v1_decoder')\n",
    "\n",
    "decoder = create_decoder(dense_shape, latent_dim=8)\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @traced\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "        -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "        axis=raxis)\n",
    "\n",
    "# @traced\n",
    "def reparameterize(mean, logvar):\n",
    "    eps = tf.random.normal(shape=mean.shape)\n",
    "    return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "# @traced\n",
    "def compute_loss(encoder, decoder, x):\n",
    "    mean, logvar = tf.split(encoder(x), num_or_size_splits=2, axis=1)\n",
    "    z = reparameterize(mean, logvar)\n",
    "    x_logit = decoder(z)\n",
    "    # tf.print(f\"x_logit, x.shape = {x_logit}, {x.shape}\")\n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "    # tf.print(f\"cross_ent.shape = {cross_ent.shape}\")\n",
    "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "@tf.function\n",
    "def train_step(encoder, decoder, x, optimizer):\n",
    "    \"\"\"Executes one training step and returns the loss.\n",
    "    This function computes the loss and gradients, and uses the latter to\n",
    "    update the model's parameters.\n",
    "    \"\"\"\n",
    "    all_trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(encoder, decoder, x)\n",
    "        # tf.print(f\"loss value = {loss}\")\n",
    "    gradients = tape.gradient(loss, all_trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, all_trainable_variables))\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "def forall_batch(input_data:np.ndarray, batch_size:int, \n",
    "            train_func:Callable[[np.ndarray],None],\n",
    "            tb_callback=None):\n",
    "    \"\"\"Batches and trains using a given function\n",
    "    \"\"\"\n",
    "    start_time = time()\n",
    "    step = 0\n",
    "    while (step+1) * batch_size < input_data.shape[0]:\n",
    "        input_batch = input_data[step*batch_size:(step+1)*batch_size,...]\n",
    "        if not (step % 10000): \n",
    "            logging.info(f\"{step}: {input_batch.shape}\")\n",
    "        if tb_callback: \n",
    "            tb_callback.on_train_batch_begin(step, logs=logs)\n",
    "\n",
    "        train_func(input_batch)\n",
    "\n",
    "        if tb_callback: \n",
    "            tb_callback.on_train_batch_end(step, logs=logs)\n",
    "        step += 1\n",
    "    end_time = time()\n",
    "    return end_time - start_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "log_dir = Path('.') / 'logs' / 'fit' / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(log_dir)\n",
    "\n",
    "tb_callback = None # tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "if tb_callback:\n",
    "    tb_callback.set_model(encoder)\n",
    "    logs = { 'loss':None, 'mean_absolute_error':None, 'output':None }\n",
    "    tb_callback.on_train_begin()\n",
    "\n",
    "batch_size = 16\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "for epoch in range(1, 121):\n",
    "    if tb_callback: \n",
    "        tb_callback.on_epoch_begin(epoch)\n",
    "\n",
    "    # perform training\n",
    "    time_elapsed = forall_batch(train_images, 16, \n",
    "                        lambda batch: train_step(encoder, decoder, batch, optimizer))\n",
    "\n",
    "    # calculate test results\n",
    "    loss = tf.keras.metrics.Mean()\n",
    "    forall_batch(test_images, 16, \n",
    "        lambda batch: loss(compute_loss(encoder, decoder, input_test_x)))\n",
    "    elbo = -loss.result()\n",
    "    logging.info(f\"Epoch: {epoch}, test ELBO: {elbo}, time elapsed: {end_time - start_time}\")\n",
    "\n",
    "    if tb_callback:\n",
    "         tb_callback.on_epoch_end(epoch)\n",
    "\n",
    "if tb_callback: \n",
    "    tb_callback.on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "test_latent = encoder(test_images[0:10,...])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "\n",
    "re_test_images = decoder(test_latent[...,0:8])\n",
    "re_test_images = sigmoid(re_test_images)\n",
    "\n",
    "quantiles = [0.05, 0.95]\n",
    "\n",
    "fig, ax = plt.subplots(2,10,figsize=(15,3))\n",
    "for n in range(10):\n",
    "    q_test = np.quantile(test_images[n],quantiles)\n",
    "    ax[0][n].imshow(test_images[n], cmap='gray', vmin=q_test[0], vmax=q_test[1])\n",
    "\n",
    "    reshape_test_image = np.reshape(re_test_images[n], (64,64))\n",
    "    q_re = np.quantile(reshape_test_image,quantiles)\n",
    "    ax[1][n].imshow(reshape_test_image, cmap='gray', vmin=q_re[0], vmax=q_re[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}